{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Error Probability and the Chernoff Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Error Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a decision function which has n inputs and 2 outputs (2 states needed to be dsitinguished using n possible measurement results).\n",
    "\n",
    "$$\n",
    "\\delta: \\{0, \\dots, n\\} \\rightarrow \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "The error probability is given by\n",
    "\n",
    "$$\n",
    "P_e(\\delta) = P(\\delta=1 | 0)p(0) + P(\\delta = 0 | 1)p(1)\n",
    "$$\n",
    "\n",
    "Using maximum posteriori decision, one can get the optimal error probability as follows\n",
    "\n",
    "$$\n",
    "P_e = \\sum_{b=1}^{n} p(b) \\min\\{p(0|b),~p(1|b)\\} = \\sum_{b=1}^{n} \\min\\{p(b|0)p(0),~p(b|1)p(1)\\}\n",
    "$$\n",
    "\n",
    "**Two problems:**\n",
    "\n",
    "1. It depends on the priori probabilities.\n",
    "2. It depends on the number of samples. If we draw two samples, we have $n^2$ incomes, i.e. $\\delta: \\{0, \\dots, n^2\\} \\rightarrow \\{0, 1\\}$.\n",
    "\n",
    "**Conclusion:** error probability is operational defined, and easy to evaluate, but is not consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Chernoff Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Theorem 2.1 Chernoff Bound**\n",
    "\n",
    "Let $P_e(N)$ be the probability of error for Bayes' decision rule after sampling one of the two distribution $p(b|0)$ or $p(b|1)$ N times. Then\n",
    "\n",
    "$$\n",
    "P_e(N) \\leq \\lambda^N\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\lambda = \\min_{0 \\leq s \\leq 1} \\sum_{b=1}^{n} p(b|0)^s p(b|1)^{1-s}\n",
    "$$\n",
    "\n",
    "This bound is approached asymptotically in the limit of large N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof 2.1 Chernoff Bound**:\n",
    "\n",
    "It can be also shown that for any two positive numbers $a$ and $b$ and any $0 \\leq s \\leq 1$, we have $\\min\\{a, b\\} \\leq a^s b^{1-s}$.\n",
    "\n",
    "The probability distributions for the outcomes of a string of N trials can be written as $p(b_1 b_2 \\dots b_N | 0)$ and  $p(b_1 b_2 \\dots b_N | 1)$. Therefore we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_e(N) &= \\sum_{b_1 b_2 \\dots b_N} \\min \\{ p(b_1 b_2 \\dots b_N | 0) p(0), p(b_1 b_2 \\dots b_N | 1) p(1)\\} \\\\\n",
    "&\\leq p(0)^s p(1)^{1-s} \\sum_{b_1 b_2 \\dots b_N} p(b_1 b_2 \\dots b_N | 0)^s p(b_1 b_2 \\dots b_N | 1)^{1-s} \\\\\n",
    "&= p(0)^s p(1)^{1-s} \\sum_{b_1 b_2 \\dots b_N} \\left( \\prod_{k=1}^{N} p(b_k|0)^s p(b_k|1)^{1-s} \\right) \\\\\n",
    "&= p(0)^s p(1)^{1-s} \\prod_{k=1}^{N} \\left(\\sum_{b_k=1}^{n} p(b_k|0)^s p(b_k|1)^{1-s} \\right) \\\\\n",
    "&= p(0)^s p(1)^{1-s} \\left(\\sum_{b_k=1}^{n} p(b_k|0)^s p(b_k|1)^{1-s} \\right)^N \\\\\n",
    "&\\leq \\left(\\sum_{b_k=1}^{n} p(b_k|0)^s p(b_k|1)^{1-s} \\right)^N\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.2 Chernoff Bound and Kullback-Leibler Divergence**\n",
    "\n",
    "The constant $\\lambda$ in the Chernoff bound can also be expressed as\n",
    "\n",
    "$$ \\lambda = K(p_{s^*}/p_0) =  K(p_{s^*}/p_1) $$\n",
    "where\n",
    "\n",
    "$$ K(p_s/p_0) = \\sum_{b=1}^{n} p_{s}(b) \\ln \\frac{p_s(b)}{p_0(b)} $$\n",
    "and\n",
    "\n",
    "$$ p_s(b) = \\frac{ p(b|0)^s p(b|1)^{1-s} }{ \\sum_b p(b|0)^s p(b|1)^{1-s} } $$\n",
    "\n",
    "$s^*$ is the optimal $s$ which achieves the minumum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Statistical Overlap / Fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Renyi divergence_ of order $\\alpha$ id defined as\n",
    "\n",
    "$$ K_\\alpha (P/Q) = \\frac{1}{\\alpha-1} \\ln \\left( \\sum_{b=1}^b p(b)^\\alpha q(b)^{1-\\alpha} \\right)$$\n",
    "\n",
    "It's bounded between 0 and 1. Setting $\\alpha=1/2$, we get dthe definition of fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Kullback-Leibler Relative Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Derivation of the Kullback-Leibler Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3 Aczel**\n",
    "\n",
    "Let $n > 3$, then the inequality\n",
    "\n",
    "$$ \\sum_{k=1}^n p_k F_k(q_k)\\le\\sum_{k=1}^n p_k F_k(p_k) $$\n",
    "\n",
    "is satisfied for all $n$-point probability distributions $(p_1,\\,\\ldots\\,,p_n)$ and $(q_1,\\,\\ldots\\,,q_n)$ if and only if there exist constants $\\alpha$ and $\\gamma_1,\\ldots,\\gamma_n$ such that\n",
    "\n",
    "$$ F_k(p)=\\alpha\\ln p+\\gamma_k\\ $$\n",
    "\n",
    "for all $k=1,2,\\ldots,n$.\n",
    "\n",
    "**Kullback-Leibler Information** (derived in the context of Honest Expert Problem)\n",
    "\n",
    "Namely, using the robust payoff function defined by Theorem Aczel we obtain\n",
    "\n",
    "$$ K(p_0/p_1) = \\sum_{b=1}^n p_0(b)\\left[\\ln p_0(b)-\\ln p_1(b)\\right] = \\sum_{b=1}^n p_0(b) \\ln \\left( \\frac{p_0(b)}{p_1(b)} \\right) $$\n",
    "\n",
    "This measure of distinguishability has appeared in other contexts and is known by various names: the Kullback-Leibler relative information, cross entropy, directed divergence, update information, and information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Properties of the Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Theorem 2.4**\n",
    "\n",
    "The most likely frequency distribution in $n$ trials is actuall the pre-assigned probability distribution.\n",
    "\n",
    "**Theorem 2.5**\n",
    "\n",
    "Suppose the experimental outcomes are described by a probability distribution  $p_0(b)$.  The probability for a particular string of outcomes $\\vec{b}\\in{\\cal T}(p_1)$ with the _wrong_ frequencies is\n",
    "\n",
    "$$\n",
    "P(\\vec{b})=\\exp\\{-n[H(p_1)+K(p_1/p_0)]\\}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H(p_1)=-\\sum_{b=1}^B p_1(b)\\ln p_1(b)\n",
    "$$\n",
    "\n",
    "is the Shannon entropy of the\n",
    "distribution $p_1(b)$.\n",
    "\n",
    "**Corollary2.1 **\n",
    "\n",
    "If the experimental outcomes are described by $p_0(b)$, the probability for a particular string $\\vec{b}\\in{\\cal T}(p_0)$ with the _correct_ frequencies is\n",
    "\n",
    "$$\n",
    "P(\\vec{b})=e^{-nH(p_0)}\n",
    "$$\n",
    "\n",
    "This follows because $K(p_0/p_0)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mutual Information$^\\dagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\dagger$ Here 'mutual information' is defined in the context of distinguishability, which is not the same with the one defined in the context of communication.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(p_0,p_1;\\pi_0,\\pi_1) &= H(\\pi_0 p_0 + \\pi_1 p_1) - \\Bigl[\\pi_0 H(p_0) + \\pi_1 H(p_1)\\Bigr] \\\\\n",
    "&= -\\sum_b p(b)\\ln p(b)+\\pi_0\\sum_b p_0(b)\\ln p_0(b)+\\pi_1\\sum_b p_1(b)\\ln p_1(b) \\\\\n",
    "&= \\pi_0\\sum_b p_0(b)\\ln\\!\\left({p_0(b)\\over p(b)}\\right) + \\pi_1\\sum_b p_1(b)\\ln\\!\\left({p_1(b)\\over p(b)}\\right) \\\\\n",
    "&= \\pi_0K(p_0/p)+\\pi_1K(p_1/p)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Distinguishability of Quantum States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Quantum Error Probability\n",
    "\n",
    "$$\n",
    "P_e(\\hat\\rho_0|\\hat\\rho_1) \\equiv \\min_{\\{\\hat E_b\\}} \\sum_b\\min\\!\\left\\{\n",
    "\\pi_0{\\rm tr}(\\hat\\rho_0\\hat E_b),\\,\\pi_1{\\rm tr}(\\hat\\rho_1\\hat E_b)\\right\\}\n",
    "$$\n",
    "\n",
    "- The Quantum Fidelity\n",
    "\n",
    "$$\n",
    "F(\\hat\\rho_0,\\hat\\rho_1)\\equiv\\min_{\\{\\hat E_b\\}}\\sum_b\n",
    "\\sqrt{{\\rm tr}(\\hat\\rho_0\\hat E_b)}\n",
    "\\sqrt{{\\rm tr}(\\hat\\rho_1\\hat E_b)}\n",
    "$$\n",
    "\n",
    "- The Quantum Renyi Overlaps\n",
    "\n",
    "$$\n",
    "F_\\alpha(\\hat\\rho_0/\\hat\\rho_1)\\equiv\\min_{\\{\\hat E_b\\}}\\sum_b\n",
    "\\left({\\rm tr}(\\hat\\rho_0\\hat E_b)\\right)^{\\!\\alpha}\\!\n",
    "\\left({\\rm tr}(\\hat\\rho_1\\hat E_b)\\right)^{\\!{1-\\alpha}},\n",
    "\\;\\;\\;\\;\\; 0<\\alpha<1\n",
    "$$\n",
    "\n",
    "- The Quantum Kullback Information\n",
    "\n",
    "$$\n",
    "K(\\hat\\rho_0/\\hat\\rho_1)\\equiv\\max_{\\{\\hat E_b\\}}\\,\n",
    "\\sum_b{\\rm tr}(\\hat\\rho_0\\hat E_b)\\,\n",
    "\\ln\\!\\left({{\\rm tr}(\\hat\\rho_0\\hat E_b)\\over\n",
    "{\\rm tr}(\\hat\\rho_1\\hat E_b)}\\right)\n",
    "$$\n",
    "\n",
    "- The Accessible Information\n",
    "\n",
    "$$\n",
    "I(\\hat\\rho_0|\\hat\\rho_1)\\equiv\\max_{\\{\\hat E_b\\}}\\,\n",
    "\\sum_b\\!\\left(\\pi_0\\,{\\rm tr}(\\hat\\rho_0\\hat E_b)\\,\n",
    "\\ln\\!\\left({{\\rm tr}(\\hat\\rho_0\\hat E_b)\\over\n",
    "{\\rm tr}(\\hat\\rho\\hat E_b)}\\right)\\,+\\,\n",
    "\\pi_1\\,{\\rm tr}(\\hat\\rho_1\\hat E_b)\\,\n",
    "\\ln\\!\\left({{\\rm tr}(\\hat\\rho_1\\hat E_b)\\over\n",
    "{\\rm tr}(\\hat\\rho\\hat E_b)}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat\\rho = \\pi_0\\hat\\rho_0+\\pi_1\\hat\\rho_1\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
